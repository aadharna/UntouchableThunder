{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, name=\"\", info=\"\", image=None):\n",
    "    \"\"\"Fn to visualize the agent playing the game in a notebook\n",
    "    \"\"\"\n",
    "    plt.figure(10)\n",
    "    plt.clf()\n",
    "    if image is not None:\n",
    "        im = image\n",
    "    else:\n",
    "        im = env.render(mode=\"rgb_array\")[0]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import optimize\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from optimization.Optimizer import PyTorchObjective\n",
    "\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from utils.utils import zelda_spaces\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "from utils.diff_evo import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es import SimpleGA, OpenES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import gvgai\n",
    "\n",
    "from generator.levels.base import Generator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ctypes as c\n",
    "\n",
    "from agent.NNagent import NNagent\n",
    "from agent.base import Agent\n",
    "\n",
    "from generator.env_gen_wrapper import GridGame\n",
    "\n",
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = NNagent(GridGame(game='dzelda', \n",
    "                      play_length=1000, \n",
    "                      path='./levels',\n",
    "                      lvl_name='1.txt',\n",
    "                      mechanics=['+', 'g'], # monsters, key, door, wall\n",
    "                      images=False,\n",
    "                  )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwwwwwwwwwwww\n",
      "w...........w\n",
      "w...........w\n",
      "w.+.........w\n",
      "w.....A.....w\n",
      "w...........w\n",
      "w...........w\n",
      "w.g.........w\n",
      "wwwwwwwwwwwww\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(_x.env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "from utils.diff_evo import differential_evolution\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('./results/'+ name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generators = [Generator(tile_world=tile(_x.env.generator.locations, *shape),\n",
    "#                        shape=shape,\n",
    "#                        path='./levels',\n",
    "#                        mechanics=['+', 'g'],\n",
    "#                        generation=0,\n",
    "#                        locations={}) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents = [NNagent(GridGame(game='dzelda', \n",
    "#                               play_length=1000, \n",
    "#                               path='./levels',\n",
    "#                               lvl_name='1.txt',\n",
    "#                               mechanics=['+', 'g'], # monsters, key, door, wall\n",
    "#                               images=False,\n",
    "#                           ), \n",
    "#                   parent=torch.load(\"./dzelda_base_agent_150.pt\")) \n",
    "#           for _ in range(5)]\n",
    "\n",
    "# for agent, gen in zip(agents, generators):\n",
    "#     agent.env.generator = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(agents[0].env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scores = {}\n",
    "# answers = {}\n",
    "\n",
    "\n",
    "# try:\n",
    "\n",
    "#     for j in range(5): # generations\n",
    "#         scores[j] = {}\n",
    "#         answers[j] = {}\n",
    "#         print('generating')\n",
    "#         for i, lvl in enumerate(generators):\n",
    "#             scores[j][i] = {}\n",
    "#             answers[j][i] = {}\n",
    "#             m, s = lvl.mutate(1)\n",
    "#             lvl.locations = m\n",
    "#             lvl.generation += 1\n",
    "#             lvl.to_file(i, game='dzelda')\n",
    "#             print(f'gen: {j}, lvl: {i}, \\n{str(lvl)}')\n",
    "\n",
    "#         print('training')\n",
    "#         for i, lvl in enumerate(generators):\n",
    "#             for a, agent in enumerate(agents):\n",
    "#                 agent.env.generator = lvl\n",
    "\n",
    "#                 objs = PyTorchObjective(agent)\n",
    "\n",
    "#                 start = time.time()\n",
    "#                 ans = differential_evolution(objs.fun, objs.bounds, \n",
    "#                                              strategy='rand1bin',\n",
    "#                                              popsize=49, \n",
    "#                                              maxiter=100,\n",
    "#                                              polish=False, \n",
    "#                                              x0=objs.x0)\n",
    "#                 end = time.time() - start\n",
    "\n",
    "#                 state_dict = objs.unpack_parameters(ans.x)\n",
    "#                 objs.f.load_state_dict(state_dict)\n",
    "\n",
    "#                 torch.save(objs.f, f'./levels/weights/weights_gen{j}_lvl{i}_agent{a}.pt')\n",
    "\n",
    "#                 # answers[generation][lvl][agentId]\n",
    "#                 answers[j][i][a] = {'ans':ans, 'agent':deepcopy(objs.f)}\n",
    "\n",
    "#         print('evaluating')\n",
    "#         # evaluate each agent on each env from this 'generation'\n",
    "#         for a, agent in enumerate(agents):\n",
    "#             # evaluate each agent with the generated levels this generation\n",
    "#             for i, lvl in enumerate(generators):\n",
    "#                 agent.env.generator = lvl\n",
    "#                 agent.nn = answers[j][i][a]['agent']\n",
    "#                 scores[j][i][a] = agent.fitness()\n",
    "\n",
    "#         save_obj(scores[j], f'gen{j}_scores')\n",
    "#         save_obj(answers[j], f'gen{j}_results')\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = PyTorchObjective(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import devo\n",
    "#import devo.jDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_gen = num_fn / popsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# # Try increasing the popsize argument by a lot. \n",
    "# result_02 = devo.jDE.run(\n",
    "#                         10000,\n",
    "#                         z.popsize,\n",
    "#                         0.5,\n",
    "#                         0.9,\n",
    "#                         z.fun_c,\n",
    "#                         z.x0.shape[0],\n",
    "#                         -5.0,\n",
    "#                         5.0,\n",
    "#                         z.create_population().ctypes.data_as(c.POINTER(c.c_double)),\n",
    "#                         z.init_fitnesses.ctypes.data_as(c.POINTER(c.c_double)),\n",
    "#                         z.results_callback\n",
    "#                         )\n",
    "\n",
    "# end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end / 3600\n",
    "\n",
    "#result = [z.fun(x, len(x)).value for x in z.out_population]\n",
    "\n",
    "#np.array(result)\n",
    "\n",
    "#z.out_fitnesses.squeeze()\n",
    "\n",
    "#z.out_fitnesses.squeeze().min()\n",
    "\n",
    "#min_result = np.argmin(z.out_fitnesses.squeeze())\n",
    "\n",
    "#best_score = z.out_fitnesses[min_result]\n",
    "\n",
    "#best_weights = z.out_population[min_result]\n",
    "\n",
    "#best_score, best_weights\n",
    "\n",
    "#z.fun(best_weights, len(best_weights))\n",
    "\n",
    "#z.x0 = best_weights\n",
    "\n",
    "#state_dict = z.unpack_parameters(z.x0)\n",
    "#z.f.load_state_dict(state_dict)\n",
    "# torch.save(z.f, f'./weights_gen{j}_lvl{i}_agent{a}.pt')\n",
    "\n",
    "#z.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DE.run(\n",
    "    max_function_evaluations,\n",
    "    population_size,\n",
    "    scaling_factor,\n",
    "    crossover_rate,\n",
    "    objective_function,\n",
    "    problem_size,\n",
    "    lower_bound,\n",
    "    upper_bound,\n",
    "    init_population,\n",
    "    init_fitnesses,\n",
    "    result_callback,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPARAMS = z.x0.shape[0]            # make this a 100-dimensinal problem.\n",
    "NPOPULATION = 32                   # use population size of 101.\n",
    "MAX_ITERATION = 10000               # run each solver for 5000 generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:             94           2          80           0          11          91\r\n",
      "Swap:            63           0          63\r\n"
     ]
    }
   ],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = cma.CMAEvolutionStrategy(NPARAMS * [1], 1, {'maxiter': MAX_ITERATION, \n",
    "#                                                  'popsize': NPOPULATION})\n",
    "\n",
    "# _scores = {}\n",
    "# _ = es.ask()\n",
    "# init_fit = evaluate(envs, objs, [obj.x0 for obj in objs])\n",
    "# es.tell([obj.x0 for obj in objs], init_fit)\n",
    "\n",
    "# es.disp_annotation()\n",
    "# for i in tqdm(range(MAX_ITERATION)):\n",
    "#     X = es.ask()\n",
    "#     results = evaluate(envs, objs, X) #vectorized evaluate of all envs\n",
    "#     _scores[i] = results\n",
    "#     es.tell(X, results)\n",
    "# es.disp()\n",
    "\n",
    "# # 4 minutes\n",
    "\n",
    "# print(np.mean(load), np.mean(game))\n",
    "\n",
    "# df = pd.DataFrame.from_dict(scores)\n",
    "# df.to_csv(f'./results/{str(es)}_fitness_scores_{MAX_ITERATION}gens.csv')\n",
    "\n",
    "# top_results = es.result\n",
    "\n",
    "# weights = top_results.xbest\n",
    "\n",
    "# np.save(f'./results/{str(es)}_best_weights.npy', np.array(weights))\n",
    "\n",
    "# im = plt.plot([score_list.max() for i, score_list in scores.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(envs, objs, population_weights):\n",
    "    # nnstart = time.time()\n",
    "    for i, obj in enumerate(objs):\n",
    "        obj.update_nn(population_weights[i])\n",
    "    # nnend = time.time() - nnstart\n",
    "    total_rewards = np.zeros(NPOPULATION)\n",
    "    obs = envs.reset()\n",
    "    \n",
    "    dones = [False]*len(objs)\n",
    "    while not np.all(dones):\n",
    "        actions = [obj.agent.get_action(obs[i]) for i, obj in enumerate(objs)]\n",
    "        envs.step_async(actions)\n",
    "        obs, rewards, dones, infos = envs.step_wait()\n",
    "        total_rewards += rewards\n",
    "    # end = time.time() - nnstart + nnend\n",
    "    return total_rewards#, end, nnend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solver(solver, seed):\n",
    "    \n",
    "    # for vectorized version, use this.\n",
    "    gyms  = [seed.env.make() for _ in tqdm(range(NPOPULATION))]    #GridGame envs\n",
    "    objs  = [PyTorchObjective(NNagent(GG=None, parent=seed.nn)) \n",
    "             for _ in tqdm(range(NPOPULATION))]                    #Objective(NNagents)\n",
    "\n",
    "    envs = SubprocVecEnv(gyms, \n",
    "                         spaces=zelda_spaces, \n",
    "                         context='fork')\n",
    "    \n",
    "    s = solver.ask()\n",
    "    s[0] = objs[0].x0\n",
    "    solver.solutions[0] = s[0]\n",
    "    init_fit = evaluate(envs, objs, s)\n",
    "    solver.tell(init_fit)\n",
    "    \n",
    "    scores = {}\n",
    "    scores[-1] = init_fit\n",
    "    \n",
    "    history = []\n",
    "    for j in tqdm(range(MAX_ITERATION)):\n",
    "        scores[j] = {}\n",
    "        solutions = solver.ask()\n",
    "        \n",
    "        fitness_list = evaluate(envs, objs, solutions)\n",
    "        \n",
    "        solver.tell(fitness_list)\n",
    "        result = solver.result() # first element is the best solution, second element is the best fitness\n",
    "        history.append(result[1])\n",
    "        \n",
    "        scores[j] = fitness_list\n",
    "        \n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"fitness at iteration\", (j+1), result[1])\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame.from_dict(scores)\n",
    "    df.to_csv(f'./results/{str(solver)}_fitness_scores_{MAX_ITERATION}gens.csv')\n",
    "    np.save(f'./results/{str(solver)}_best_weights.npy', result[0])\n",
    "\n",
    "    return history, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines genetic algorithm solver\n",
    "ga = SimpleGA(NPARAMS,                # number of model parameters\n",
    "               sigma_init=0.5,        # initial standard deviation\n",
    "               popsize=NPOPULATION,   # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               weight_decay=0.00,     # weight decay coefficient\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 86258.18it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1170.42it/s]\n",
      "  1%|          | 100/10000 [27:37<45:06:12, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness at iteration 100 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 163/10000 [2:06:04<343:54:38, 125.86s/it]"
     ]
    }
   ],
   "source": [
    "ga_history, result = test_solver(ga, _x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ga_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(ga_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oes = OpenES(NPARAMS,                  # number of model parameters\n",
    "            sigma_init=0.5,            # initial standard deviation\n",
    "            sigma_decay=0.999,         # don't anneal standard deviation\n",
    "            learning_rate=0.1,         # learning rate for standard deviation\n",
    "            learning_rate_decay = 1.0, # annealing the learning rate\n",
    "            popsize=NPOPULATION,       # population size\n",
    "            antithetic=False,          # whether to use antithetic sampling\n",
    "            weight_decay=0.00,         # weight decay coefficient\n",
    "            rank_fitness=False,        # use rank rather than fitness numbers\n",
    "            forget_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oes_history, result = test_solver(oes, _x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oes_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oes_res = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es import CMAES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaes = CMAES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              weight_decay=0.0,\n",
    "              sigma_init = 0.5\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cma_history, cma_result = test_solver(cmaes, _x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cma_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma_res = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.fun(z.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.x0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.x0 = ans.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = z.unpack_parameters(ans.x)\n",
    "z.f.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.f == _x.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.prev_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.orientation[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.get_action(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents[3].env.generator = generators[2]\n",
    "t = agents[3].env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(action):\n",
    "    a, b, c, d = agents[3].env.step(action)\n",
    "    im = d['pic']\n",
    "    return im, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, r = move(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = _x.fitness(fn=show_state) if _x.env.pics else _x.fitness()\n",
    "_x.vis=None\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('winning_with_monster.npy', _x.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1600 * 1000 * 100 # frames seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(_x.nn, \"./dzelda_base_agent_150.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "objs = []\n",
    "answers = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    t.append(NNagent(GridGame(game='dzelda', \n",
    "                      play_length=1000, \n",
    "                      path='./levels',\n",
    "                      lvl_name=f'{i}.txt',\n",
    "                      mechanics=['1', '2', '3', '+', 'g', 'w'], # monsters, key, door, wall\n",
    "                      images=False,\n",
    "                             )\n",
    "                    )\n",
    "            )\n",
    "    \n",
    "    if i == 1:\n",
    "        t[-1].nn = torch.load(\"./dzelda_base_agent_150.pt\")\n",
    "    else:\n",
    "        t[-1].nn = torch.load(f\"./dzelda_{i-1}_agent_{150*(i)}.pt\") #load previous best weights\n",
    "    \n",
    "    objs.append(PyTorchObjective(t[-1]))\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    ans = differential_evolution(objs[-1].fun, bounds, \n",
    "                                 strategy='rand1bin',\n",
    "                                 popsize=99, \n",
    "                                 maxiter=150,\n",
    "                                 polish=False, \n",
    "                                 x0=objs[-1].x0)\n",
    "    end = time.time() - start\n",
    "    \n",
    "    answers.append(ans)\n",
    "    state_dict = objs[-1].unpack_parameters(ans.x)\n",
    "    objs[-1].f.load_state_dict(state_dict)\n",
    "    \n",
    "    torch.save(t[-1].nn, f\"./dzelda_{i}_agent_{150*(i+1)}.pt\")\n",
    "    \n",
    "for pair in t:\n",
    "    pair.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "First: Train an agent on an extremely simple level to give the agent a behavior starting point. \n",
    "    - In POET that was a flat terrain. In my case, it's a level that looks like this: \n",
    "    -   wwwwwwwwwwwww    \n",
    "        w...........w    A --> agent\n",
    "        w...........w    + --> key\n",
    "        w.+....A....w    g --> goal\n",
    "        w...........w\n",
    "        w...........w    Task: Take the key to the goal\n",
    "        w...........w\n",
    "        w.g.........w\n",
    "        wwwwwwwwwwwww\n",
    "        \n",
    "Second: Initialize agent-environment population with the first learned behavior\n",
    "    \n",
    "While True:\n",
    "    \n",
    "    Evaluate each agent in it's paired environment\n",
    "    \n",
    "    Mutate environments (every m loops). \n",
    "    \n",
    "        Mutation of an environment causes the agent neural network to be copied into the new environment\n",
    "        This increases the population.\n",
    "        \n",
    "        - An example mutation could be\n",
    "            - adding/removing in an enemy (three types)\n",
    "            - adding/moving a goal\n",
    "            - adding/removing a key\n",
    "            - moving an agent\n",
    "            - An example new level could look like this: \n",
    "                - wwwwwwwwwwwww    \n",
    "                  w....+..1...w    A --> agent\n",
    "                  w...g.......w    + --> key\n",
    "                  w...........w    g --> goal\n",
    "                  w...........w\n",
    "                  w...w.......w    Task: Take the key to the goal\n",
    "                  w.......A...w\n",
    "                  w.g.........w\n",
    "                  wwwwwwwwwwwww\n",
    "\n",
    "    (slowly) Run one step of optimization for each agent within it's paired environment.\n",
    "    \n",
    "    Transfer agents between environments (every k loops)\n",
    "        Intuition: Agent alpha might have learned behavior in it's paired environment that is actually behavior that is very good in environment beta. \n",
    "        \n",
    "        - test every agent in every environment. \n",
    "        - transfer into environment i, the agent j, who performed the best.\n",
    "    \n",
    "    Return to top of the loop.\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note. \n",
    "\n",
    "Moving the key from top right corner to mid left created a slightly simpler env.\n",
    "\n",
    "----  \n",
    "\n",
    "we were not able to learn the good policy if we kept the wieght range as [-2, 2]. \n",
    "\n",
    "Next I am retrying the same starting point but with range [-5, 5]. --> solved extremely simple env with this range and simpler env. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Then moving the key up one spot meant that the agent needs more training. So far it has failed to take the key and get to the goal after an additional 20 generations of training (but does get the key). I am giving it another 20 generations. \n",
    "\n",
    "After the agent learns the new environment (key moved up one spot), I am going to take those weights and put them back into the first env (key moved back down one spot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Note: There are times that the optimization straight up fails after only a generation or two. I think this is coming from the fact that the problem is very sparely rewarded.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited dzelda.txt: \n",
    "    - picking up key +1\n",
    "    - killing monster +1\n",
    "    - taking key to door +2\n",
    "\n",
    "\n",
    "1)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+....A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "2)   \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "3)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "4)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.w.......w\n",
    "w...w..A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "```  \n",
    "   Net(\n",
    "      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (fc1): Linear(in_features=96, out_features=48, bias=True)\n",
    "      (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
    "      (fc3): Linear(in_features=24, out_features=6, bias=True)\n",
    "   )\n",
    "\n",
    "```\n",
    "\n",
    "# Differetial Evolution:\n",
    "\n",
    "## $\\theta :=$ model_weights  \n",
    "## Pick $\\theta_a, \\theta_b, \\theta_c$   \n",
    "## $Proposal_\\theta = \\theta_a + \\alpha * (\\theta_b - \\theta_c))$\n",
    "\n",
    "## Pros:\n",
    "Computationally efficient  \n",
    "Self-adaptation and crossover due to $\\theta_b - \\theta_c$\n",
    "\n",
    "## Problems:  \n",
    "\n",
    "$\\theta$ is a ~10000 dimensional vector. \n",
    "\n",
    "Curse of Dimensionality!   \n",
    "    - As the dimension go up, vectors become equidistant  \n",
    "\n",
    "Good weight configurations are sparse.\n",
    "\n",
    "Rewards are sparse.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting observation. Whenever the agent completes the goal, it seems to do so with the minimal path. That's suprising to me because we're giving the agent 1000 time-steps and the fitness function is not taking account (yet) of the number of steps that the agent has used as a weighting on the score it achieves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
