{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, name=\"\", info=\"\", image=None):\n",
    "    \"\"\"Fn to visualize the agent playing the game in a notebook\n",
    "    \"\"\"\n",
    "    plt.figure(10)\n",
    "    plt.clf()\n",
    "    if image is not None:\n",
    "        im = image\n",
    "    else:\n",
    "        im = env.render(mode=\"rgb_array\")[0]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import optimize\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from optimization.Optimizer import PyTorchObjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gvgai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator.levels.base import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.NNagent import NNagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator.env_gen_wrapper import GridGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = NNagent(GridGame(game='dzelda', \n",
    "                      play_length=1000, \n",
    "                      path='./levels',\n",
    "                      lvl_name='3.txt',\n",
    "                      mechanics=['1', '2', '3', '+', 'g', 'w'], # monsters, key, door, wall\n",
    "                      images=False,\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(_x.env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _x.nn = torch.load(\"./25_gen_weights_5_5.pt\")\n",
    "# _x.nn = torch.load(\"./1_monster_weights_5_5.pt\")\n",
    "_x.nn = torch.load(\"./dzelda_2_agent_450.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = PyTorchObjective(_x)\n",
    "# z = PyTorchObjective(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.fun(z.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.x0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(-5, 5)]*z.x0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.diff_evo import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: positive values being shown as the acheived score is BAD! \n",
    "# We're trying to minimize the loss surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ans = differential_evolution(z.fun, bounds, \n",
    "                             strategy='rand1bin',\n",
    "                             popsize=99, \n",
    "                             maxiter=150,\n",
    "                             polish=False, \n",
    "                             x0=z.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end//3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.x0 = ans.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = z.unpack_parameters(ans.x)\n",
    "z.f.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.f == _x.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = _x.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(action):\n",
    "    a, b, c, d = _x.env.step(action)\n",
    "    im = d['pic']\n",
    "    return im, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im, r = move(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = _x.fitness(fn=show_state) if _x.env.pics else _x.fitness()\n",
    "_x.vis=None\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2600 * 1000 * 100 # frames seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(_x.nn, \"./dzelda_3_agent_600.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "objs = []\n",
    "answers = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    t.append(NNagent(GridGame(game='dzelda', \n",
    "                      play_length=1000, \n",
    "                      path='./levels',\n",
    "                      lvl_name=f'{i}.txt',\n",
    "                      mechanics=['1', '2', '3', '+', 'g', 'w'], # monsters, key, door, wall\n",
    "                      images=False,\n",
    "                             )\n",
    "                    )\n",
    "            )\n",
    "    \n",
    "    if i == 1:\n",
    "        t[-1].nn = torch.load(\"./dzelda_base_agent_150.pt\")\n",
    "    else:\n",
    "        t[-1].nn = torch.load(f\"./dzelda_{i-1}_agent_{150*(i)}.pt\") #load previous best weights\n",
    "    \n",
    "    objs.append(PyTorchObjective(t[-1]))\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    ans = differential_evolution(objs[-1].fun, bounds, \n",
    "                                 strategy='rand1bin',\n",
    "                                 popsize=99, \n",
    "                                 maxiter=150,\n",
    "                                 polish=False, \n",
    "                                 x0=objs[-1].x0)\n",
    "    end = time.time() - start\n",
    "    \n",
    "    answers.append(ans)\n",
    "    state_dict = objs[-1].unpack_parameters(ans.x)\n",
    "    objs[-1].f.load_state_dict(state_dict)\n",
    "    \n",
    "    torch.save(t[-1].nn, f\"./dzelda_{i}_agent_{150*(i+1)}.pt\")\n",
    "    \n",
    "for pair in t:\n",
    "    pair.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "First: Train an agent on an extremely simple level to give the agent a behavior starting point. \n",
    "    - In POET that was a flat terrain. In my case, it's a level that looks like this: \n",
    "    -   wwwwwwwwwwwww    \n",
    "        w...........w    A --> agent\n",
    "        w...........w    + --> key\n",
    "        w.+....A....w    g --> goal\n",
    "        w...........w\n",
    "        w...........w    Task: Take the key to the goal\n",
    "        w...........w\n",
    "        w.g.........w\n",
    "        wwwwwwwwwwwww\n",
    "        \n",
    "Second: Initialize agent-environment population with the first learned behavior\n",
    "    \n",
    "While True:\n",
    "    \n",
    "    Evaluate each agent in it's paired environment\n",
    "    \n",
    "    Mutate environments (every m loops). \n",
    "    \n",
    "        Mutation of an environment causes the agent neural network to be copied into the new environment\n",
    "        This increases the population.\n",
    "        \n",
    "        - An example mutation could be\n",
    "            - adding/removing in an enemy (three types)\n",
    "            - adding/moving a goal\n",
    "            - adding/removing a key\n",
    "            - moving an agent\n",
    "            - An example new level could look like this: \n",
    "                - wwwwwwwwwwwww    \n",
    "                  w....+..1...w    A --> agent\n",
    "                  w...g.......w    + --> key\n",
    "                  w...........w    g --> goal\n",
    "                  w...........w\n",
    "                  w...w.......w    Task: Take the key to the goal\n",
    "                  w.......A...w\n",
    "                  w.g.........w\n",
    "                  wwwwwwwwwwwww\n",
    "\n",
    "    (slowly) Run one step of optimization for each agent within it's paired environment.\n",
    "    \n",
    "    Transfer agents between environments (every k loops)\n",
    "        Intuition: Agent alpha might have learned behavior in it's paired environment that is actually behavior that is very good in environment beta. \n",
    "        \n",
    "        - test every agent in every environment. \n",
    "        - transfer into environment i, the agent j, who performed the best.\n",
    "    \n",
    "    Return to top of the loop.\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note. \n",
    "\n",
    "Moving the key from top right corner to mid left created a slightly simpler env.\n",
    "\n",
    "----  \n",
    "\n",
    "we were not able to learn the good policy if we kept the wieght range as [-2, 2]. \n",
    "\n",
    "Next I am retrying the same starting point but with range [-5, 5]. --> solved extremely simple env with this range and simpler env. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Then moving the key up one spot meant that the agent needs more training. So far it has failed to take the key and get to the goal after an additional 20 generations of training (but does get the key). I am giving it another 20 generations. \n",
    "\n",
    "After the agent learns the new environment (key moved up one spot), I am going to take those weights and put them back into the first env (key moved back down one spot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Note: There are times that the optimization straight up fails after only a generation or two. I think this is coming from the fact that the problem is very sparely rewarded.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited dzelda.txt: \n",
    "    - picking up key +1\n",
    "    - killing monster +1\n",
    "    - taking key to door +2\n",
    "\n",
    "\n",
    "1)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+....A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "2)   \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "3)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "4)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.w.......w\n",
    "w...w..A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "```  \n",
    "   Net(\n",
    "      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (fc1): Linear(in_features=96, out_features=48, bias=True)\n",
    "      (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
    "      (fc3): Linear(in_features=24, out_features=6, bias=True)\n",
    "   )\n",
    "\n",
    "```\n",
    "\n",
    "# Differetial Evolution:\n",
    "\n",
    "## $\\theta :=$ model_weights  \n",
    "## Pick $\\theta_a, \\theta_b, \\theta_c$   \n",
    "## $Proposal_\\theta = \\theta_a + \\alpha * (\\theta_b - \\theta_c))$\n",
    "\n",
    "## Pros:\n",
    "Computationally efficient  \n",
    "Self-adaptation and crossover due to $\\theta_b - \\theta_c$\n",
    "\n",
    "## Problems:  \n",
    "\n",
    "$\\theta$ is a ~10000 dimensional vector. \n",
    "\n",
    "Curse of Dimensionality!   \n",
    "    - As the dimension go up, vectors become equidistant  \n",
    "\n",
    "Good weight configurations are sparse.\n",
    "\n",
    "Rewards are sparse.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting observation. Whenever the agent completes the goal, it seems to do so with the minimal path. That's suprising to me because we're giving the agent 1000 time-steps and the fitness function is not taking account (yet) of the number of steps that the agent has used as a weighting on the score it achieves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
