{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, name=\"\", info=\"\"):\n",
    "    \"\"\"Fn to visualize the agent playing the game in a notebook\n",
    "    \"\"\"\n",
    "    plt.figure(10)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import optimize\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "class PyTorchObjective(object):\n",
    "    \"\"\"PyTorch objective function, wrapped to be called by scipy.optimize.\"\"\"\n",
    "    def __init__(self, agent):\n",
    "        self.f = agent.nn # some pytorch module, that produces a scalar loss\n",
    "        # make an x0 from the parameters in this module\n",
    "        parameters = OrderedDict(agent.nn.named_parameters())\n",
    "        self.param_shapes = {n:parameters[n].size() for n in parameters}\n",
    "        # ravel and concatenate all parameters to make x0\n",
    "        self.x0 = np.concatenate([parameters[n].data.numpy().ravel() \n",
    "                                   for n in parameters])\n",
    "        \n",
    "        self.eval_fn = agent.evaluate\n",
    "        self.c = 0\n",
    "\n",
    "    def unpack_parameters(self, x):\n",
    "        \"\"\"optimize.minimize will supply 1D array, chop it up for each parameter.\"\"\"\n",
    "        i = 0\n",
    "        named_parameters = OrderedDict()\n",
    "        for n in self.param_shapes:\n",
    "            param_len = reduce(lambda x,y: x*y, self.param_shapes[n])\n",
    "            # slice out a section of this length\n",
    "            param = x[i:i+param_len]\n",
    "            # reshape according to this size, and cast to torch\n",
    "            param = param.reshape(*self.param_shapes[n])\n",
    "            named_parameters[n] = torch.from_numpy(param)\n",
    "            # update index\n",
    "            i += param_len\n",
    "        return named_parameters\n",
    "\n",
    "    def pack_grads(self):\n",
    "        \"\"\"pack all the gradients from the parameters in the module into a\n",
    "        numpy array.\"\"\"\n",
    "        grads = []\n",
    "        for p in self.f.parameters():\n",
    "            grad = p.grad.data.numpy()\n",
    "            grads.append(grad.ravel())\n",
    "        return np.concatenate(grads)\n",
    "\n",
    "    def is_new(self, x):\n",
    "        # if this is the first thing we've seen\n",
    "        if not hasattr(self, 'cached_x'):\n",
    "            return True\n",
    "        else:\n",
    "            # compare x to cached_x to determine if we've been given a new input\n",
    "            x, self.cached_x = np.array(x), np.array(self.cached_x)\n",
    "            error = np.abs(x - self.cached_x)\n",
    "            return error.max() > 1e-8\n",
    "\n",
    "    def cache(self, x):\n",
    "        # unpack x and load into module \n",
    "        state_dict = self.unpack_parameters(x)\n",
    "        self.f.load_state_dict(state_dict)\n",
    "        # store the raw array as well\n",
    "        self.cached_x = x\n",
    "        # zero the gradient\n",
    "        self.f.zero_grad()\n",
    "        # use it to calculate the objective\n",
    "        score = self.eval_fn()\n",
    "        self.cached_score = score\n",
    "\n",
    "    def fun(self, x):\n",
    "        self.c += 1\n",
    "        if self.is_new(x):\n",
    "            self.cache(x)\n",
    "        if self.c % 10 == 0:\n",
    "            print(f\"achieved score of: {self.cached_score} on {self.c}\")\n",
    "        return self.cached_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gvgai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator.levels.base import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.NNagent import NNagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator.env_gen_wrapper import GridGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = NNagent(GridGame(game='zelda', \n",
    "                     play_length=1000, \n",
    "                     lvl_name='test0.txt',\n",
    "                     mechanics=['1', '2', '3', '+', 'g', 'w'], # monsters, key, door, wall\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwwwwwwwwwwww\n",
      "w...........w\n",
      "w...........w\n",
      "w.+...A.....w\n",
      "w...........w\n",
      "w...........w\n",
      "w...........w\n",
      "w.g.........w\n",
      "wwwwwwwwwwwww\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(_x.env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _x.nn = torch.load(\"./25_gen_weights_5_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=96, out_features=48, bias=True)\n",
       "  (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (fc3): Linear(in_features=24, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x.env.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = PyTorchObjective(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9262,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.x0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(-5, 5)]*z.x0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.diff_evo import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "achieved score of: -0.0 on 10\n",
      "achieved score of: -0.0 on 20\n",
      "achieved score of: -0.0 on 30\n",
      "achieved score of: -0.0 on 40\n",
      "achieved score of: -1.0 on 50\n",
      "achieved score of: -0.0 on 60\n",
      "achieved score of: -0.0 on 70\n",
      "achieved score of: -0.0 on 80\n",
      "achieved score of: -1.0 on 90\n",
      "achieved score of: -0.0 on 100\n",
      "achieved score of: -0.0 on 110\n",
      "achieved score of: -1.0 on 120\n",
      "achieved score of: -1.0 on 130\n",
      "achieved score of: -0.0 on 140\n",
      "achieved score of: -0.0 on 150\n",
      "achieved score of: -0.0 on 160\n",
      "achieved score of: -0.0 on 170\n",
      "achieved score of: -0.0 on 180\n",
      "achieved score of: -0.0 on 190\n",
      "achieved score of: -0.0 on 200\n",
      "achieved score of: -0.0 on 210\n",
      "achieved score of: -0.0 on 220\n",
      "achieved score of: -0.0 on 230\n",
      "achieved score of: -0.0 on 240\n",
      "achieved score of: -0.0 on 250\n",
      "achieved score of: -0.0 on 260\n",
      "achieved score of: -0.0 on 270\n",
      "achieved score of: -0.0 on 280\n",
      "achieved score of: -0.0 on 290\n",
      "achieved score of: -0.0 on 300\n",
      "achieved score of: -0.0 on 310\n",
      "achieved score of: -1.0 on 320\n",
      "achieved score of: -0.0 on 330\n",
      "achieved score of: -0.0 on 340\n",
      "achieved score of: -1.0 on 350\n",
      "achieved score of: -0.0 on 360\n",
      "achieved score of: -0.0 on 370\n",
      "achieved score of: -0.0 on 380\n",
      "achieved score of: -0.0 on 390\n",
      "achieved score of: -1.0 on 400\n",
      "achieved score of: -0.0 on 410\n",
      "achieved score of: -0.0 on 420\n",
      "achieved score of: -0.0 on 430\n",
      "achieved score of: -0.0 on 440\n",
      "achieved score of: -0.0 on 450\n",
      "achieved score of: -0.0 on 460\n",
      "achieved score of: -0.0 on 470\n",
      "achieved score of: -1.0 on 480\n",
      "achieved score of: -1.0 on 490\n",
      "achieved score of: -0.0 on 500\n",
      "achieved score of: -0.0 on 510\n",
      "achieved score of: -1.0 on 520\n",
      "achieved score of: -0.0 on 530\n",
      "achieved score of: -0.0 on 540\n",
      "achieved score of: -0.0 on 550\n",
      "achieved score of: -1.0 on 560\n",
      "achieved score of: -0.0 on 570\n",
      "achieved score of: -1.0 on 580\n",
      "achieved score of: -0.0 on 590\n",
      "achieved score of: -0.0 on 600\n",
      "achieved score of: -0.0 on 610\n",
      "achieved score of: -0.0 on 620\n",
      "achieved score of: -0.0 on 630\n",
      "achieved score of: -0.0 on 640\n",
      "achieved score of: -0.0 on 650\n",
      "achieved score of: -0.0 on 660\n",
      "achieved score of: -0.0 on 670\n",
      "achieved score of: -0.0 on 680\n",
      "achieved score of: -0.0 on 690\n",
      "achieved score of: -0.0 on 700\n",
      "achieved score of: -0.0 on 710\n",
      "achieved score of: -0.0 on 720\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ans = differential_evolution(z.fun, bounds, \n",
    "                             strategy='rand1bin',\n",
    "                             popsize=99, \n",
    "                             maxiter=25,\n",
    "                             polish=False, \n",
    "                             x0=z.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end = time.time() - start\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.x0 = ans.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = z.unpack_parameters(ans.x)\n",
    "z.f.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.f == _x.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _x.fitness(fn=show_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _x.vis=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(_x.nn, \"./25_gen_weights_5_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note. \n",
    "\n",
    "Moving the key from top right corner to mid left created a slightly simpler env.\n",
    "\n",
    "----  \n",
    "\n",
    "we were not able to learn the good policy if we kept the wieght range as [-2, 2]. \n",
    "\n",
    "Next I am retrying the same starting point but with range [-5, 5]. --> solved extremely simple env with this range and simpler env. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Then moving the key up one spot meant that the agent needs more training. So far it has failed to take the key and get to the goal after an additional 20 generations of training (but does get the key). I am giving it another 20 generations. \n",
    "\n",
    "After the agent learns the new environment (key moved up one spot), I am going to take those weights and put them back into the first env (key moved back down one spot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
