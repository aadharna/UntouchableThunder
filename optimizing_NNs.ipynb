{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, name=\"\", info=\"\", image=None):\n",
    "    \"\"\"Fn to visualize the agent playing the game in a notebook\n",
    "    \"\"\"\n",
    "    plt.figure(10)\n",
    "    plt.clf()\n",
    "    if image is not None:\n",
    "        im = image\n",
    "    else:\n",
    "        im = env.render(mode=\"rgb_array\")[0]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import optimize\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from optimization.Optimizer import PyTorchObjective\n",
    "\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from utils.utils import zelda_spaces\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es import SimpleGA, OpenES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import gvgai\n",
    "\n",
    "from generator.levels.base import Generator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ctypes as c\n",
    "\n",
    "from agent.NNagent import NNagent\n",
    "from agent.base import Agent\n",
    "\n",
    "from generator.env_gen_wrapper import GridGame\n",
    "\n",
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = NNagent(time_stamp=int(time.time()),\n",
    "             prefix='.',\n",
    "             GG=GridGame(game='dzelda', \n",
    "                          play_length=1000, \n",
    "                          path='./levels',\n",
    "                          lvl_name='4.txt',\n",
    "                          mechanics=['+', 'g'], # monsters, key, door, wall\n",
    "                          images=False\n",
    "                    )\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(_x.env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from utils.diff_evo import differential_evolution\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('./results/'+ name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generators = [Generator(tile_world=tile(_x.env.generator.locations, *shape),\n",
    "#                        shape=shape,\n",
    "#                        path='./levels',\n",
    "#                        mechanics=['+', 'g'],\n",
    "#                        generation=0,\n",
    "#                        locations={}) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents = [NNagent(GridGame(game='dzelda', \n",
    "#                               play_length=1000, \n",
    "#                               path='./levels',\n",
    "#                               lvl_name='1.txt',\n",
    "#                               mechanics=['+', 'g'], # monsters, key, door, wall\n",
    "#                               images=False,\n",
    "#                           ), \n",
    "#                   parent=torch.load(\"./dzelda_base_agent_150.pt\")) \n",
    "#           for _ in range(5)]\n",
    "\n",
    "# for agent, gen in zip(agents, generators):\n",
    "#     agent.env.generator = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(agents[0].env.generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scores = {}\n",
    "# answers = {}\n",
    "\n",
    "\n",
    "# try:\n",
    "\n",
    "#     for j in range(5): # generations\n",
    "#         scores[j] = {}\n",
    "#         answers[j] = {}\n",
    "#         print('generating')\n",
    "#         for i, lvl in enumerate(generators):\n",
    "#             scores[j][i] = {}\n",
    "#             answers[j][i] = {}\n",
    "#             m, s = lvl.mutate(1)\n",
    "#             lvl.locations = m\n",
    "#             lvl.generation += 1\n",
    "#             lvl.to_file(i, game='dzelda')\n",
    "#             print(f'gen: {j}, lvl: {i}, \\n{str(lvl)}')\n",
    "\n",
    "#         print('training')\n",
    "#         for i, lvl in enumerate(generators):\n",
    "#             for a, agent in enumerate(agents):\n",
    "#                 agent.env.generator = lvl\n",
    "\n",
    "#                 objs = PyTorchObjective(agent)\n",
    "\n",
    "#                 start = time.time()\n",
    "#                 ans = differential_evolution(objs.fun, objs.bounds, \n",
    "#                                              strategy='rand1bin',\n",
    "#                                              popsize=49, \n",
    "#                                              maxiter=100,\n",
    "#                                              polish=False, \n",
    "#                                              x0=objs.x0)\n",
    "#                 end = time.time() - start\n",
    "\n",
    "#                 state_dict = objs.unpack_parameters(ans.x)\n",
    "#                 objs.f.load_state_dict(state_dict)\n",
    "\n",
    "#                 torch.save(objs.f, f'./levels/weights/weights_gen{j}_lvl{i}_agent{a}.pt')\n",
    "\n",
    "#                 # answers[generation][lvl][agentId]\n",
    "#                 answers[j][i][a] = {'ans':ans, 'agent':deepcopy(objs.f)}\n",
    "\n",
    "#         print('evaluating')\n",
    "#         # evaluate each agent on each env from this 'generation'\n",
    "#         for a, agent in enumerate(agents):\n",
    "#             # evaluate each agent with the generated levels this generation\n",
    "#             for i, lvl in enumerate(generators):\n",
    "#                 agent.env.generator = lvl\n",
    "#                 agent.nn = answers[j][i][a]['agent']\n",
    "#                 scores[j][i][a] = agent.fitness()\n",
    "\n",
    "#         save_obj(scores[j], f'gen{j}_scores')\n",
    "#         save_obj(answers[j], f'gen{j}_results')\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = PyTorchObjective(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devo\n",
    "import devo.jDE\n",
    "import devo.CoDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_gen = num_fn / popsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Try increasing the popsize argument by a lot. \n",
    "result_02 = devo.jDE.run(\n",
    "                            100000,\n",
    "                            z.popsize,\n",
    "                            0.5,\n",
    "                            0.1,\n",
    "                            z.fun_c,\n",
    "                            z.x0.shape[0],\n",
    "                            -5.0,\n",
    "                            5.0,\n",
    "                            z.create_population().ctypes.data_as(c.POINTER(c.c_double)),\n",
    "                            z.init_fitnesses.ctypes.data_as(c.POINTER(c.c_double)),\n",
    "                            z.results_callback\n",
    "                            )\n",
    "\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end // 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = GridGame(game='dzelda', \n",
    "                      play_length=1000, \n",
    "                      path='./levels',\n",
    "                      lvl_name='4.txt',\n",
    "                      mechanics=['+', 'g'], # monsters, key, door, wall\n",
    "                      images=True,\n",
    "                  )\n",
    "\n",
    "nn2 = NNagent(env2)\n",
    "\n",
    "_x2 = PyTorchObjective(nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x2.update_nn(z.best_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x2.agent.fitness(fn=show_state)\n",
    "_x2.agent.fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./results/EC/jDE_{10000}gen_lvl4_best_weights.npy', z.best_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(_x2.agent.nn, f'./results/EC/jDE_{10000}gen_lvl4_best_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DE.run(\n",
    "    max_function_evaluations,\n",
    "    population_size,\n",
    "    scaling_factor,\n",
    "    crossover_rate,\n",
    "    objective_function,\n",
    "    problem_size,\n",
    "    lower_bound,\n",
    "    upper_bound,\n",
    "    init_population,\n",
    "    init_fitnesses,\n",
    "    result_callback,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPARAMS = z.x0.shape[0]            # make this a 100-dimensinal problem.\n",
    "NPOPULATION = 32                   # use population size of 101.\n",
    "MAX_ITERATION = 10000              # run each solver for 5000 generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(envs, objs, population_weights):\n",
    "    # nnstart = time.time()\n",
    "    for i, obj in enumerate(objs):\n",
    "        obj.update_nn(population_weights[i])\n",
    "    # nnend = time.time() - nnstart\n",
    "    total_rewards = np.zeros(NPOPULATION)\n",
    "    obs = envs.reset()\n",
    "    \n",
    "    # openAI vec-envs will automatically restart an env\n",
    "    # if env_a has finished but env_b has not.\n",
    "    # This is good behavior for RL, but BAD behavior for EC. \n",
    "    # So, we'll mask out envs-that-have-finished of the total_reward computation.\n",
    "    mask  = np.zeros(len(objs), dtype=bool)\n",
    "    while not np.all(mask):\n",
    "        actions = [obj.agent.get_action(obs[i]) for i, obj in enumerate(objs)]\n",
    "        envs.step_async(actions)\n",
    "        obs, rewards, dones, infos = envs.step_wait()\n",
    "        \n",
    "        # if the env has not finished, add its reward information\n",
    "        # to the to the total_reward/fitness vector.\n",
    "        #  If the env has finished once, update the mask\n",
    "        #  and then use that mask to zero-out further rewards. \n",
    "        total_rewards += ~mask * rewards\n",
    "        \n",
    "        # update mask at index where an env has finished once.\n",
    "        for i, d in enumerate(dones):\n",
    "            if d and mask[i] == False:\n",
    "                mask[i] = d\n",
    "\n",
    "    # end = time.time() - nnstart + nnend\n",
    "    return total_rewards#, end, nnend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solver(solver, seed):\n",
    "    date = datetime.today().strftime('%Y_%m_%d')\n",
    "    sol_name = str(solver).split(' ')[0][1:]\n",
    "    tempdir = f'./results/EC/{sol_name}_{MAX_ITERATION}gens_{date}'\n",
    "    tmp = os.path.join(tempdir, 'tmp')\n",
    "    if not os.path.exists(tempdir):\n",
    "        os.mkdir(tempdir)\n",
    "        os.mkdir(tmp)\n",
    "    \n",
    "    # for vectorized version, use this.\n",
    "    gyms  = [seed.env.make() for _ in tqdm(range(solver.popsize))]    #GridGame envs\n",
    "    objs  = [PyTorchObjective(NNagent(GG=None, parent=seed.nn)) \n",
    "             for _ in tqdm(range(solver.popsize))]                    #Objective(NNagents)\n",
    "\n",
    "    envs = SubprocVecEnv(gyms, \n",
    "                         spaces=zelda_spaces, \n",
    "                         context='fork')\n",
    "    \n",
    "    print(\"make vec envs\")\n",
    "    s = solver.ask()\n",
    "    print(\"asked solver\")\n",
    "    s[0] = objs[0].x0\n",
    "    solver.solutions[0] = s[0]\n",
    "    init_fit = evaluate(envs, objs, s)\n",
    "    print('evaluated solver')\n",
    "    solver.tell(init_fit)\n",
    "    print('told solver')\n",
    "    scores = {}\n",
    "    scores[-1] = init_fit\n",
    "    \n",
    "    history = []\n",
    "    print('starting looping')\n",
    "    for j in tqdm(range(MAX_ITERATION)):\n",
    "        scores[j] = {}\n",
    "        solutions = solver.ask()\n",
    "        \n",
    "        fitness_list = evaluate(envs, objs, solutions)\n",
    "\n",
    "        solver.tell(fitness_list)\n",
    "        result = solver.result() # first element is the best solution, second element is the best fitness\n",
    "        history.append(result[1])\n",
    "        \n",
    "        scores[j] = fitness_list\n",
    "        \n",
    "        if (j+1) % 500 == 0:\n",
    "            print(\"fitness at iteration\", (j+1), result[1])\n",
    "            tmp_scores = pd.DataFrame.from_dict(scores)\n",
    "            tmp_scores.to_csv(os.path.join(tmp, \n",
    "                                           f'{sol_name}_fitness_scores_{j+1}_of_{MAX_ITERATION}_{date}.csv'))\n",
    "            np.save(os.path.join(tmp, \n",
    "                                 f'{sol_name}_best_weights_gen_{j+1}_{date}.npy'), result[0])\n",
    "            del tmp_scores\n",
    "            gc.collect()\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame.from_dict(scores)\n",
    "    df.to_csv(os.path.join(tempdir, \n",
    "                           f'{sol_name}_{date}_fitness_scores_{MAX_ITERATION}gens.csv'))\n",
    "    np.save(os.path.join(tempdir, \n",
    "                         f'{sol_name}_{date}_best_weights_{MAX_ITERATION}gens.npy'), result[0])\n",
    "    np.save(os.path.join(tempdir, \n",
    "                         f'{sol_name}_{date}_{MAX_ITERATION}history.npy'), history)\n",
    "\n",
    "    envs.close()\n",
    "    \n",
    "    return history, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es import CMAES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from es import PEPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepg = PEPG(NPARAMS,             # number of model parameters\n",
    "               sigma_init=0.10,              # initial standard deviation\n",
    "               sigma_alpha=0.20,             # learning rate for standard deviation\n",
    "               sigma_decay=0.999,            # anneal standard deviation\n",
    "               sigma_limit=0.01,             # stop annealing if less than this\n",
    "               sigma_max_change=0.2,         # clips adaptive sigma to 20%\n",
    "               learning_rate=0.01,           # learning rate for standard deviation\n",
    "               learning_rate_decay = 0.9999, # annealing the learning rate\n",
    "               learning_rate_limit = 0.01,   # stop annealing learning rate\n",
    "               elite_ratio = 0,              # if > 0, then ignore learning_rate\n",
    "               popsize=NPOPULATION,          # population size\n",
    "               average_baseline=True,        # set baseline to average of batch\n",
    "               weight_decay=0.01,            # weight decay coefficient\n",
    "               rank_fitness=False,           # use rank rather than fitness numbers\n",
    "               forget_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pepg_history, pepg_result = test_solver(pepg, _x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pepg_history)\n",
    "plt.savefig('./results/es.PEPG_10000gens_2020_01_05/history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepg_res = pepg_result[0]\n",
    "z.update_nn(pepg_res)\n",
    "z.eval_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink network?\n",
    "# neat\n",
    "\n",
    "cmaes = CMAES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              weight_decay=0.0,\n",
    "              sigma_init = 0.05, #shrink this.\n",
    "              diag=True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cma_history, cma_result = test_solver(cmaes, _x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cma_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma_res = cma_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.update_nn(cma_res)\n",
    "z.eval_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defines genetic algorithm solver\n",
    "# ga = SimpleGA(NPARAMS,                # number of model parameters\n",
    "#                sigma_init=0.5,        # initial standard deviation\n",
    "#                popsize=NPOPULATION,   # population size\n",
    "#                elite_ratio=0.1,       # percentage of the elites\n",
    "#                forget_best=False,     # forget the historical best elites\n",
    "#                weight_decay=0.00,     # weight decay coefficient\n",
    "#               )\n",
    "\n",
    "# # STILL RUNNING, BUT IS NOT UPDATING THE GUI. \n",
    "# ## Please don't touch.\n",
    "\n",
    "# ga_history, result = test_solver(ga, _x)\n",
    "\n",
    "# plt.plot(ga_history)\n",
    "\n",
    "# result[0]\n",
    "\n",
    "# z.update_nn(result[0])\n",
    "\n",
    "# z.eval_fn()\n",
    "\n",
    "# oes = OpenES(NPARAMS,                  # number of model parameters\n",
    "#             sigma_init=0.5,            # initial standard deviation\n",
    "#             sigma_decay=0.999,         # don't anneal standard deviation\n",
    "#             learning_rate=0.1,         # learning rate for standard deviation\n",
    "#             learning_rate_decay = 1.0, # annealing the learning rate\n",
    "#             popsize=NPOPULATION,       # population size\n",
    "#             antithetic=False,          # whether to use antithetic sampling\n",
    "#             weight_decay=0.00,         # weight decay coefficient\n",
    "#             rank_fitness=False,        # use rank rather than fitness numbers\n",
    "#             forget_best=False)\n",
    "\n",
    "# oes_history, result = test_solver(oes, _x)\n",
    "\n",
    "# plt.plot(oes_history)\n",
    "\n",
    "# oes_res = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = _x.fitness(fn=show_state) if _x.env.pics else _x.fitness()\n",
    "_x.vis=None\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('winning_with_monster.npy', _x.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1600 * 1000 * 100 # frames seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(_x.nn, \"./dzelda_base_agent_150.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "First: Train an agent on an extremely simple level to give the agent a behavior starting point. \n",
    "    - In POET that was a flat terrain. In my case, it's a level that looks like this: \n",
    "    -   wwwwwwwwwwwww    \n",
    "        w...........w    A --> agent\n",
    "        w...........w    + --> key\n",
    "        w.+....A....w    g --> goal\n",
    "        w...........w\n",
    "        w...........w    Task: Take the key to the goal\n",
    "        w...........w\n",
    "        w.g.........w\n",
    "        wwwwwwwwwwwww\n",
    "        \n",
    "Second: Initialize agent-environment population with the first learned behavior\n",
    "    \n",
    "While True:\n",
    "    \n",
    "    Evaluate each agent in it's paired environment\n",
    "    \n",
    "    Mutate environments (every m loops). \n",
    "    \n",
    "        Mutation of an environment causes the agent neural network to be copied into the new environment\n",
    "        This increases the population.\n",
    "        \n",
    "        - An example mutation could be\n",
    "            - adding/removing in an enemy (three types)\n",
    "            - adding/moving a goal\n",
    "            - adding/removing a key\n",
    "            - moving an agent\n",
    "            - An example new level could look like this: \n",
    "                - wwwwwwwwwwwww    \n",
    "                  w....+..1...w    A --> agent\n",
    "                  w...g.......w    + --> key\n",
    "                  w...........w    g --> goal\n",
    "                  w...........w\n",
    "                  w...w.......w    Task: Take the key to the goal\n",
    "                  w.......A...w\n",
    "                  w.g.........w\n",
    "                  wwwwwwwwwwwww\n",
    "\n",
    "    (slowly) Run one step of optimization for each agent within it's paired environment.\n",
    "    \n",
    "    Transfer agents between environments (every k loops)\n",
    "        Intuition: Agent alpha might have learned behavior in it's paired environment that is actually behavior that is very good in environment beta. \n",
    "        \n",
    "        - test every agent in every environment. \n",
    "        - transfer into environment i, the agent j, who performed the best.\n",
    "    \n",
    "    Return to top of the loop.\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note. \n",
    "\n",
    "Moving the key from top right corner to mid left created a slightly simpler env.\n",
    "\n",
    "----  \n",
    "\n",
    "we were not able to learn the good policy if we kept the wieght range as [-2, 2]. \n",
    "\n",
    "Next I am retrying the same starting point but with range [-5, 5]. --> solved extremely simple env with this range and simpler env. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Then moving the key up one spot meant that the agent needs more training. So far it has failed to take the key and get to the goal after an additional 20 generations of training (but does get the key). I am giving it another 20 generations. \n",
    "\n",
    "After the agent learns the new environment (key moved up one spot), I am going to take those weights and put them back into the first env (key moved back down one spot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "Note: There are times that the optimization straight up fails after only a generation or two. I think this is coming from the fact that the problem is very sparely rewarded.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited dzelda.txt: \n",
    "    - picking up key +1\n",
    "    - killing monster +1\n",
    "    - taking key to door +2\n",
    "\n",
    "\n",
    "1)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+....A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "2)   \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w...........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "3)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.........w\n",
    "w......A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```\n",
    "\n",
    "4)  \n",
    "```\n",
    "wwwwwwwwwwwww\n",
    "w...........w\n",
    "w...........w\n",
    "w.+.w.......w\n",
    "w...w..A....w\n",
    "w...........w\n",
    "w..1........w\n",
    "w.g.........w\n",
    "wwwwwwwwwwwww\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "```  \n",
    "   Net(\n",
    "      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (fc1): Linear(in_features=96, out_features=48, bias=True)\n",
    "      (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
    "      (fc3): Linear(in_features=24, out_features=6, bias=True)\n",
    "   )\n",
    "\n",
    "```\n",
    "\n",
    "# Differetial Evolution:\n",
    "\n",
    "## $\\theta :=$ model_weights  \n",
    "## Pick $\\theta_a, \\theta_b, \\theta_c$   \n",
    "## $Proposal_\\theta = \\theta_a + \\alpha * (\\theta_b - \\theta_c))$\n",
    "\n",
    "## Pros:\n",
    "Computationally efficient  \n",
    "Self-adaptation and crossover due to $\\theta_b - \\theta_c$\n",
    "\n",
    "## Problems:  \n",
    "\n",
    "$\\theta$ is a ~10000 dimensional vector. \n",
    "\n",
    "Curse of Dimensionality!   \n",
    "    - As the dimension go up, vectors become equidistant  \n",
    "\n",
    "Good weight configurations are sparse.\n",
    "\n",
    "Rewards are sparse.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting observation. Whenever the agent completes the goal, it seems to do so with the minimal path. That's suprising to me because we're giving the agent 1000 time-steps and the fitness function is not taking account (yet) of the number of steps that the agent has used as a weighting on the score it achieves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
